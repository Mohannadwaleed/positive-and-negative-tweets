{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BnJsjXrbtBEO",
    "outputId": "c10d3284-de15-4c24-90c9-fa978625f1f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.rnn_models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_encoded_tweets\n",
    "from sklearn.rnn_model_selection import train_test_split\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "tweets = positive_tweets + negative_tweets\n",
    "sentiments = [1] * len(positive_tweets) + [0] * len(negative_tweets)  # 1 for positive, 0 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DdTtemjoxJxs"
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000  # Maximum number of words in the vocabulary\n",
    "sequence_length = 50      # Maximum length of encoded_tweets\n",
    "\n",
    "text_processor = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")  # Handle out-of-vocabulary words\n",
    "text_processor.fit_on_tweets(tweets)\n",
    "encoded_tweets = text_processor.tweets_to_encoded_tweets(tweets)\n",
    "vocab_index = text_processor.vocab_index\n",
    "\n",
    "padded_tweets = pad_encoded_tweets(encoded_tweets, maxlen=sequence_length)\n",
    "sentiments = np.array(sentiments)\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(padded_tweets, sentiments, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiqiXu2VxLTN",
    "outputId": "c80749f0-c94a-40d0-9b0f-61daf282fc83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_size = 50  # Size of the word embeddings\n",
    "\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=sequence_length),\n",
    "\n",
    "    SimpleRNN(32, return_encoded_tweets=True),\n",
    "\n",
    "    SimpleRNN(32, return_encoded_tweets=True),\n",
    "\n",
    "    SimpleRNN(64, return_encoded_tweets=False),\n",
    "\n",
    "    Dropout(0.3),  # Dropout layer for regularization\n",
    "\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),  # Fully connected layer\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paPiL_82xRAj",
    "outputId": "f5084e58-d69b-4288-9ba9-4cd108c2f870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 25ms/step - accuracy: 0.5320 - loss: 1.1444 - val_accuracy: 0.6925 - val_loss: 0.7151\n",
      "Epoch 2/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8027 - loss: 0.5540 - val_accuracy: 0.7631 - val_loss: 0.5530\n",
      "Epoch 3/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9305 - loss: 0.2367 - val_accuracy: 0.7588 - val_loss: 0.6353\n",
      "Epoch 4/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.9707 - loss: 0.1190 - val_accuracy: 0.7569 - val_loss: 0.7227\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.7544 - loss: 0.5762\n",
      "Test Accuracy: 0.7735000252723694\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "train_history = rnn_model.fit(train_features, train_labels,\n",
    "                    epochs=30,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "test_loss, test_accuracy = rnn_model.evaluate(test_features, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
